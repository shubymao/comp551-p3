{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project 3 Notebook.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "deepnote": {
      "is_reactive": false
    },
    "deepnote_execution_queue": [
      {
        "cellId": "00014-5023c705-89c0-4b0d-8475-31dbf86965f1",
        "msgId": "df859eb8-5087-492b-81a0-bf32da9d72c2",
        "sessionId": "ba7e9811-adf0-4ae0-bc6e-78a462d11bf3"
      }
    ],
    "deepnote_notebook_id": "6ceacadf-544f-46ff-b007-fbadf7aa1867",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shubymao/comp551-p3/blob/vic/Project_3_Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Msqh4iDyCudk"
      },
      "source": [
        "# Project 3 - Convolution Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CpzLeM6RCudt"
      },
      "source": [
        "## Team Members"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjQkokOPCudu"
      },
      "source": [
        "Le-Li Mao (260800098)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "diF0MA9uCudu"
      },
      "source": [
        "Victor Livernoche (260926276)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXsMnElHCudv"
      },
      "source": [
        "Enan Ashaduzzaman (260805923)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXOuUvc6Cudw"
      },
      "source": [
        "## Initializing Dependency"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RNubcKypCudx",
        "outputId": "4270026a-34d4-448c-8acc-c871d5cd06bf"
      },
      "source": [
        "from __future__ import print_function\n",
        "import pickle as pkl\n",
        "import numpy as np\n",
        "import argparse\n",
        "from tqdm import tqdm\n",
        "from sklearn.preprocessing import normalize\n",
        "\n",
        "import cv2\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "import pandas as pd\n",
        "\n",
        "%matplotlib inline\n",
        "from mpl_toolkits.axes_grid1 import ImageGrid\n",
        "import matplotlib.pyplot as plt\n",
        "import string\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhtudTKhCudz"
      },
      "source": [
        "## Loading Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nTc9y2J_Cud0"
      },
      "source": [
        "mapping = list(string.ascii_lowercase) \n",
        "training_data, training_labels, testing_data = None, None, None\n",
        "with open(\"/content/drive/My Drive/data/images_l.pkl\", 'rb') as f: \n",
        "    training_data = pkl.load(f)\n",
        "with open(\"/content/drive/My Drive/data/labels_l.pkl\", 'rb') as f: \n",
        "    training_labels = pkl.load(f)\n",
        "with open(\"/content/drive/My Drive/data/images_test.pkl\", 'rb') as f: \n",
        "    testing_data = pkl.load(f)\n",
        "with open(\"/content/drive/My Drive/data/images_ul.pkl\", 'rb') as f: \n",
        "    unlabeled_data = pkl.load(f)"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6d_j7rMFpZg"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UflUy3ApCud1"
      },
      "source": [
        "Create the pipeline for handling data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCWo0YwkNHWs"
      },
      "source": [
        "def rotate_image(image, angle):\n",
        "  image_center = tuple(np.array(image.shape[1::-1]) / 2)\n",
        "  rot_mat = cv2.getRotationMatrix2D(image_center, angle, 1.0)\n",
        "  result = cv2.warpAffine(image, rot_mat, image.shape[1::-1], flags=cv2.INTER_LINEAR)\n",
        "  return result"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHRptvAyNmEe"
      },
      "source": [
        "def nomalize_img(img):\n",
        "  cliped_img = np.clip(img, 0, 255)\n",
        "  subtracted_img = cliped_img - cliped_img.mean() + 10\n",
        "  normalized_img = subtracted_img / np.max(subtracted_img) * 255\n",
        "  return normalized_img"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EA5ehjMbynFA"
      },
      "source": [
        "def extract_key_item(original, padding=2, threshold=100 , show_all = False, debug=False):\n",
        "  stages = None\n",
        "  img = cv2.normalize(original, None, 0, 255, cv2.NORM_MINMAX, cv2.CV_8U)\n",
        "  #img = cv2.fastNlMeansDenoising(img, None, 3, 3)\n",
        "  img_blur = cv2.GaussianBlur(img, (7, 7), 0)\n",
        "  (_, thresh) = cv2.threshold(img_blur, 65, 255, cv2.THRESH_BINARY)\n",
        "  if debug: plt.imshow(img_blur); plt.show()\n",
        "  if debug: plt.imshow(thresh); plt.show()\n",
        "  contours, _ = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)\n",
        "  key_items = []\n",
        "  bounding_box_cords = []\n",
        "  for c in contours:\n",
        "      x,y,w,h = cv2.boundingRect(c)\n",
        "      if show_all or debug: cv2.rectangle(img, (x, y), (x + w, y + h), 255, 1)\n",
        "      if(w < 10 and h < 10): # select elements with at least 10px width or 10 px height\n",
        "        continue\n",
        "      dup = False\n",
        "      for (px, py) in bounding_box_cords:\n",
        "        if abs(px-x)+abs(py-y) < 10:\n",
        "          dup = True\n",
        "      if dup: continue   \n",
        "      key_items.append(original[y-padding:y+h+padding,x-padding:x+w+padding].copy())\n",
        "      \n",
        "      bounding_box_cords.append((x,y,))\n",
        "  if show_all: \n",
        "    stages = [img_blur, thresh, img]\n",
        "  if debug: plt.imshow(img); plt.show()\n",
        "  return stages, key_items"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fZppzL12QhU"
      },
      "source": [
        "def pad_to_fixed_size(img, width, height):\n",
        "  row, col = img.shape\n",
        "  if row > height or col > width: \n",
        "    return None\n",
        "  pl = (width - col)// 2\n",
        "  pt = (height - row)// 2\n",
        "  pr = (width - col - pl)\n",
        "  pb = (height - row - pt)\n",
        "  return np.pad(img, [(pt,pb),(pl,pr)], 'constant')"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ufgde1f40_Mc"
      },
      "source": [
        "def merge_key_items(key_items):\n",
        "  if len(key_items) >= 2:\n",
        "    item1 = pad_to_fixed_size(key_items[0],28,56)\n",
        "    item2 = pad_to_fixed_size(key_items[1],28,56)\n",
        "    if item1 is None or item2 is None: return None\n",
        "    result = np.concatenate((item1,item2), axis = 1)\n",
        "    return result \n",
        "  return None"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hkoF49-AGwy_"
      },
      "source": [
        "def pre_processing(original, show_all=False, debug=False):\n",
        "  normalized_img = nomalize_img(original.copy())\n",
        "  if debug: plt.imshow(normalized_img); plt.show()\n",
        "  extract_stages, key_items = extract_key_item(normalized_img, show_all=show_all, debug=debug)\n",
        "  if debug: [(plt.imshow(item), plt.show()) for item in key_items]\n",
        "  result = merge_key_items(key_items)\n",
        "  #result = cv2.normalize(result, None, 0, 255, cv2.NORM_MINMAX, cv2.CV_8U)\n",
        "  #result = cv2.fastNlMeansDenoising(result, None, 3, 7)\n",
        "  #result = cv2.threshold(result, 65, 255, cv2.THRESH_BINARY)\n",
        "  #result = np.asarray(result)\n",
        "  if result is None: return normalized_img, None, False\n",
        "  stages = None\n",
        "  if show_all:\n",
        "    stages = [original, normalized_img, *extract_stages, *key_items, result]\n",
        "  return result, stages, True"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5_VqihgmpXu"
      },
      "source": [
        "def convert_labels_to_readables(labels):\n",
        "  readables = []\n",
        "  for label in labels:\n",
        "    l1 = np.argmax(label[0:10])\n",
        "    l2 = mapping[np.argmax(label[10:])]\n",
        "    readables.append((l1,l2,))\n",
        "  return readables"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67D6zKQtpG8h"
      },
      "source": [
        "# show images\n",
        "def img_grid(row,col,imgs):\n",
        "  if imgs is None: return\n",
        "  for r in range(row):\n",
        "    fig, axs = plt.subplots(nrows=1, ncols=col, figsize=(7.,10.))\n",
        "    for i, ax in enumerate(axs.flatten()):\n",
        "        if(len(imgs) <= r*col+i): \n",
        "          return\n",
        "        plt.sca(ax)\n",
        "        plt.imshow(imgs[r*col+i])\n",
        "        #plt.colorbar()\n",
        "    plt.show()"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "id": "uPnJcsozCud3",
        "outputId": "d2eaa4d5-15f3-4ad3-b254-8ecafebe1b3b"
      },
      "source": [
        "# Test the preprocessing functions\n",
        "for ind in np.random.choice(training_data.shape[0], 3, replace=False):\n",
        "  img = training_data[ind]\n",
        "  _, stages,_ = pre_processing(img, show_all=True)\n",
        "  if stages is not None: img_grid(1,len(stages),stages)"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-93-a34072780261>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstages\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpre_processing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_all\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m   \u001b[0;32mif\u001b[0m \u001b[0mstages\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mimg_grid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-92-aa986a2b3715>\u001b[0m in \u001b[0;36mimg_grid\u001b[0;34m(row, col, imgs)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mimg_grid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mimgs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mncols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m7.\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'int' object is not callable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBJcLOQAXrKY"
      },
      "source": [
        "# transform all data to the output format\n",
        "def process_data(data):\n",
        "  output, bad_index = [], []\n",
        "  for i in range(len(data)):\n",
        "      result,_,valid = pre_processing(data[i])\n",
        "      output.append(result)\n",
        "      if not valid:\n",
        "        bad_index.append(i)\n",
        "  output = np.array(output).astype('float64')\n",
        "  return output, bad_index\n",
        "\n",
        "def normalize_data(data):\n",
        "  output = []\n",
        "  for i in range(len(data)):\n",
        "      output.append(nomalize_img(data[i].copy()))\n",
        "  output = np.array(output).astype('float64')\n",
        "  return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ntjjm_ejHulW"
      },
      "source": [
        "normalized_training_data = normalize_data(training_data)\n",
        "processed_training_data, bad_training_index = process_data(training_data)\n",
        "\n",
        "print(f'Number of training data element unable to be separated: {len(bad_training_index)}')\n",
        "processed_training_data = np.delete(processed_training_data, bad_training_index, axis=0)\n",
        "processed_training_labels = np.delete(training_labels, bad_training_index, axis=0)\n",
        "# We append the processed to original as additional training data\n",
        "final_training_data = np.append(normalized_training_data, processed_training_data, axis=0)\n",
        "final_training_labels = np.append(training_labels, processed_training_labels, axis=0)\n",
        "\n",
        "print(final_training_data.shape)\n",
        "print(final_training_labels.shape)\n",
        "\n",
        "normalized_unlabeled_data = normalize_data(unlabeled_data)\n",
        "processed_unlabeled_data, bad_unlabeled_index = process_data(unlabeled_data)\n",
        "\n",
        "print(f'Number of training data element unable to be separated: {len(bad_unlabeled_index)}')\n",
        "processed_unlabeled_data = np.delete(processed_unlabeled_data, bad_unlabeled_index, axis=0)\n",
        "# We append the processed to original as additional training data\n",
        "\n",
        "final_unlabeled_data = np.append(normalized_unlabeled_data, processed_unlabeled_data, axis=0)\n",
        "\n",
        "print(final_unlabeled_data.shape)\n",
        "\n",
        "# Clean up to reduce memory footprint\n",
        "training_data, processed_training_data, normalized_training_data = None, None, None\n",
        "training_labels, processed_training_labels = None, None\n",
        "unlabeled_data, processes_unlabeled_data, normalized_unlabeled_data = None, None, None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "326fPZePHvh9"
      },
      "source": [
        "processed_testing_data, bad_testing_index = process_data(testing_data)\n",
        "print(f'Number of test data element unable to be processed: {len(bad_testing_index)}')\n",
        "print(np.shape(processed_testing_data))\n",
        "testing_data = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uy1lkBOmbE9w"
      },
      "source": [
        "def show_random_sample(n):\n",
        "  index = np.random.choice(final_training_data.shape[0], n, replace=False)  \n",
        "  img_grid(n//5,5,final_training_data[index])\n",
        "  print(convert_labels_to_readables(final_training_labels[index]))\n",
        "show_random_sample(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eo0NbFKGJDbi"
      },
      "source": [
        "Put the data into dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ef_-4cWkCud2"
      },
      "source": [
        "training_tensor = torch.from_numpy(final_training_data[:,None]).float()\n",
        "number_label_tensor = torch.from_numpy(np.argmax(final_training_labels[:,0:10],1))\n",
        "letter_label_tensor = torch.from_numpy(np.argmax(final_training_labels[:,10:36],1))\n",
        "multi_label_tensor = torch.stack((number_label_tensor,letter_label_tensor), axis=1)\n",
        "\n",
        "# mixed_label_tensor = torch.cat((number_label_tensor, letter_label_tensor), 0)\n",
        "# mixed_training_tensor = torch.cat((training_tensor, training_tensor), 0)\n",
        "# mixed_training_dataset = TensorDataset(mixed_training_tensor, mixed_label_tensor)\n",
        "# mixed_training_dataloader = DataLoader(mixed_training_dataset, batch_size=30, shuffle=True, num_workers=2)\n",
        "\n",
        "multi_training_dataset = TensorDataset(training_tensor, multi_label_tensor)\n",
        "#multi_training_dataloader = DataLoader(multi_training_dataset, batch_size=30, shuffle=True, num_workers=2)\n",
        "\n",
        "train_size = int(0.9*len(multi_training_dataset))\n",
        "validation_size = len(multi_training_dataset)-train_size\n",
        "\n",
        "multi_training_dataset, multi_validation_dataset = random_split(multi_training_dataset, [train_size, validation_size])\n",
        "multi_training_dataloader = DataLoader(multi_training_dataset, batch_size=30, shuffle=True, num_workers=2)\n",
        "multi_validation_dataloader = DataLoader(multi_validation_dataset, batch_size=30, shuffle=True, num_workers=2)\n",
        "\n",
        "training_tensor, number_label_tensor, multi_label_tensor = None, None, None\n",
        "final_training_data, final_training_labels = None, None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wL3AbTECud4"
      },
      "source": [
        "## Training Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_VOUOUHcFZW"
      },
      "source": [
        "Initialize network validation and training function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AwkzEV1q8TNm"
      },
      "source": [
        "def validate_model(model, validation_dataloader):\n",
        "  digit_acc, letter_acc = 0, 0\n",
        "  for i, validation_data in enumerate(tqdm(validation_dataloader)):\n",
        "    inputs, labels = validation_data\n",
        "    o1, o2 = model(inputs)\n",
        "    l1, l2 = labels[:,0], labels[:,1]\n",
        "    digit_acc += torch.mean((torch.argmax(o1, dim=1) == l1).float()).float()\n",
        "    letter_acc += torch.mean((torch.argmax(o2, dim=1) == l2).float())\n",
        "  digit_acc = digit_acc/len(validation_dataloader)\n",
        "  letter_acc = letter_acc / len(validation_dataloader)\n",
        "  return (digit_acc.item(), letter_acc.item())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7Bsqq4ECud6"
      },
      "source": [
        "def train_network(net, dataloader, validation_dataloader, lr = 0.005, momentum = 0.9, max_epoch = 7, criterion = nn.CrossEntropyLoss()):\n",
        "  optimizer = optim.SGD(net.parameters(), lr=lr, momentum=momentum)\n",
        "  running_loss = 0.0\n",
        "  digits_accs, letter_accs = [], []\n",
        "  for epoch in range(max_epoch):  # loop over the dataset multiple times\n",
        "      cnt = 0\n",
        "      for i, data in enumerate(tqdm(dataloader)):\n",
        "          inputs, labels = data\n",
        "          optimizer.zero_grad()\n",
        "          outputs = net(inputs)\n",
        "          loss = criterion(outputs, labels)\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          running_loss += loss.item()\n",
        "          cnt+=1\n",
        "      # print training statistics\n",
        "      print('[%d] loss: %.3f' % (epoch + 1, running_loss / cnt))\n",
        "      running_loss = 0.0\n",
        "      digit_acc, letter_acc = validate_model(net, validation_dataloader)\n",
        "      print(digit_acc)\n",
        "      print(letter_acc)\n",
        "      digits_accs.append(digit_acc)\n",
        "      letter_accs.append(letter_acc)\n",
        "  return (digits_accs, letter_accs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Hjn-Gkl09aY"
      },
      "source": [
        "### Multi-Output Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XVNq7TyM3nc"
      },
      "source": [
        "Initalize the Multilabel Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mzXNmXUTM870"
      },
      "source": [
        "class MultiNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MultiNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 64, 6, padding=1)\n",
        "        self.conv2 = nn.Conv2d(64, 256, 4, padding=1, stride=1)\n",
        "        self.conv3 = nn.Conv2d(256, 256, 3, padding='same')\n",
        "        self.conv4 = nn.Conv2d(256, 256, 3, padding='same')\n",
        "        self.conv5 = nn.Conv2d(256, 128, 3, padding='same')\n",
        "        # 3 Dense Layer\n",
        "        self.fc11 = nn.Linear(3200, 2000)  \n",
        "        self.fc21 = nn.Linear(3200, 2000)  \n",
        "        self.fc12 = nn.Linear(2000, 1000)\n",
        "        self.fc22 = nn.Linear(2000, 1200)\n",
        "        self.fc13 = nn.Linear(1000, 10)\n",
        "        self.fc23 = nn.Linear(1200, 26)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.max_pool2d(F.relu(self.conv1(x)), 3)\n",
        "        x = F.max_pool2d(F.relu(self.conv2(x)), 3)\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = F.relu(self.conv4(x))\n",
        "        x = F.relu(self.conv5(x))\n",
        "        # print(x.size())\n",
        "        x = x.view(-1,self.num_flat_features(x)) # Flatten\n",
        "        \n",
        "        x1 = F.relu(self.fc11(x))\n",
        "        x1 = F.relu(self.fc12(x1))\n",
        "        l1 = self.fc13(x1)\n",
        "\n",
        "        x2 = self.fc21(x)\n",
        "        x2 = self.fc22(x2)\n",
        "        l2 = self.fc23(x2)\n",
        "        return l1,l2\n",
        "    \n",
        "    def num_flat_features(self, x):\n",
        "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
        "        num_features = 1\n",
        "        for s in size:\n",
        "            num_features *= s\n",
        "        return num_features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-nTa_yc4NHi3"
      },
      "source": [
        "def train_model(dataloader, epoch):\n",
        "  multi_net = MultiNet()\n",
        "  multi_net_criterion = nn.CrossEntropyLoss()\n",
        "  lr = 0.001\n",
        "  def ml_criterion(output, labels):\n",
        "    o1, o2 = output\n",
        "    l1, l2 = labels[:,0], labels[:,1]\n",
        "    loss1 = multi_net_criterion(o1,l1)\n",
        "    loss2 = multi_net_criterion(o2,l2)\n",
        "    return loss1 + loss2\n",
        "  res = train_network(multi_net, dataloader, multi_validation_dataloader, criterion=ml_criterion, lr=lr, max_epoch=epoch)\n",
        "  digit_accuracies, letter_accuracies = res\n",
        "  plt.plot(range(1, epoch+1), digit_accuracies, 'bo-')\n",
        "  plt.plot(range(1, epoch+1), letter_accuracies, 'ro-')\n",
        "  return multi_net\n",
        "multi_net = train_model(multi_training_dataloader, 6)\n",
        "# save to download if it crashes\n",
        "from os.path import exists\n",
        "torch.save(multi_net.state_dict(), 'checkpoint.pth')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDb5UK-aTPrf"
      },
      "source": [
        "# find labels on unlabeled data\n",
        "state_dict = torch.load('checkpoint.pth')\n",
        "multi_net = MultiNet()\n",
        "multi_net.load_state_dict(state_dict)\n",
        "\n",
        "labels = []\n",
        "k = 0\n",
        "while(k<60000):\n",
        "  r = k+500\n",
        "  if r > len(final_unlabeled_data):\n",
        "    r = len(final_unlabeled_data)\n",
        "  l1,l2 = multi_net(torch.from_numpy(final_unlabeled_data[k:r, None]).float())\n",
        "  l1 = np.argmax(l1.detach().numpy(), axis=1)\n",
        "  l2 = np.argmax(l2.detach().numpy(), axis=1)\n",
        "  for j in range(0, len(l1)):\n",
        "    labels.append([l1[j], l2[j]])\n",
        "    k += 1\n",
        "unlabeled_labels = np.array(labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYeBkYkCWWRm"
      },
      "source": [
        "for img in unlabeled_data[0:3, :]:\n",
        "  _, stages,_ = pre_processing(img, show_all=True)\n",
        "  if stages is not None: img_grid(1,len(stages),stages)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RclGfKrKWSeg"
      },
      "source": [
        "unlabeled_tensor = torch.from_numpy(final_unlabeled_data[:,None]).float()\n",
        "\n",
        "processed_unlabeled_labels = np.delete(unlabeled_labels, bad_unlabeled_index, axis=0)\n",
        "final_unlabeled_labels = np.append(unlabeled_labels, processed_unlabeled_labels, axis=0)\n",
        "\n",
        "unlabeled_dataset = TensorDataset(unlabeled_tensor, torch.from_numpy(final_unlabeled_labels))\n",
        "final_training_dataset = torch.utils.data.ConcatDataset([multi_training_dataset, unlabeled_dataset])\n",
        "\n",
        "final_training_dataloader = DataLoader(final_training_dataset, batch_size=30, shuffle=True, num_workers=2)\n",
        "# train with the training and unlabeled data\n",
        "multi_net = train_model(final_training_dataloader, 6)\n",
        "from os.path import exists\n",
        "torch.save(multi_net.state_dict(), 'checkpoint2.pth')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qTAPelZCud7"
      },
      "source": [
        "## Testing Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VyHpvnsdJXN8"
      },
      "source": [
        "Test the multi output network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RvF7XMKAJb0o"
      },
      "source": [
        "img_grid(2,5,final_training_data[200:210])\n",
        "l1,l2 = multi_net(torch.from_numpy(final_training_data[200:210,None]).float())\n",
        "letter_prediction = list(map(lambda x: mapping[x], np.argmax(l2.detach().numpy(),axis=1)))\n",
        "prediction = list(zip(np.argmax(l1.detach().numpy(),axis=1),letter_prediction))\n",
        "print(prediction)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_lNcU_CP009V"
      },
      "source": [
        "!pip install scikit-learn\n",
        "from sklearn.metrics import confusion_matrix\n",
        "validation_data, validation_labels = multi_validation_dataset[:]\n",
        "\n",
        "l1,l2 = multi_net(validation_data)\n",
        "\n",
        "accuracy_digits = np.mean(np.argmax(l1.detach().numpy(), axis=1)==validation_labels.detach().numpy()[:, 0])\n",
        "accuracy_letters = np.mean(np.argmax(l2.detach().numpy(), axis=1)==validation_labels.detach().numpy()[:, 1])\n",
        "#accuracy = np.mean(np.all(np.argmax(l1.detach().numpy(), axis=1)==validation_labels.detach().numpy()[:, 0], np.argmax(l2.detach().numpy(), axis=1)==validation_labels.detach().numpy()[:, 1]))\n",
        "print(confusion_matrix(validation_labels.detach().numpy()[:, 0], np.argmax(l1.detach().numpy(), axis=1)))\n",
        "print(confusion_matrix(validation_labels.detach().numpy()[:, 1], np.argmax(l2.detach().numpy(), axis=1)))\n",
        "\n",
        "print(accuracy_digits)\n",
        "print(accuracy_letters)\n",
        "#print(accuracy)\n",
        "#and np.argmax(l2.detach().numpy(), axis=1)==np.argmax(final_training_labels[None, 11:27], axis=1)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4VlH7E76Cud7"
      },
      "source": [
        "## Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iB7GDwKTNFOd"
      },
      "source": [
        "def LabelToString(label, isDigit):\n",
        "  s = \"\"\n",
        "  r = 0\n",
        "  if (isDigit):\n",
        "    r = 10\n",
        "  else:\n",
        "    r = 26\n",
        "  for i in range(0, r):\n",
        "    if (i == label):\n",
        "      s += '1'\n",
        "    else:\n",
        "      s += '0'\n",
        "  return s\n",
        "\n",
        "category = []\n",
        "k = 0\n",
        "while(k<15000):\n",
        "  l1,l2 = multi_net(torch.from_numpy(processed_testing_data[k:k+100, None]).float())\n",
        "  l1 = np.argmax(l1.detach().numpy(), axis=1)\n",
        "  l2 = np.argmax(l2.detach().numpy(), axis=1)\n",
        "  for i in range(0, len(l1)):\n",
        "    a = LabelToString(l1[i], True)\n",
        "    b = LabelToString(l2[i], False)\n",
        "    category.append(a+b)\n",
        "    k += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxYZsufbCud8"
      },
      "source": [
        "id = []\n",
        "for i in range(0, 15000):\n",
        "  id.append(i)\n",
        "# need to create data dict\n",
        "data = {\n",
        "    '# Id' : id,\n",
        "    'Category' : category\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data=data, columns= ['# Id', 'Category'])\n",
        "print(df)\n",
        "df.to_csv('/content/drive/My Drive/data/results.csv', index = False, header=True)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}